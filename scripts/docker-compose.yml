services:
  kafka:
    user: "0:0"
    image: confluentinc/cp-kafka:8.1.0
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092"
      - "9094:9094"
    environment:
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_NODE_ID: "1"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"

      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,EXTERNAL://0.0.0.0:9094,CONTROLLER://0.0.0.0:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092,EXTERNAL://127.0.0.1:9094"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"

      KAFKA_NUM_PARTITIONS: "1"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: "0"

      KAFKA_MESSAGE_MAX_BYTES: "10485760"
      KAFKA_REPLICA_FETCH_MAX_BYTES: "10485760"

      KAFKA_LOG_DIRS: "/var/lib/kafka/data"
      CLUSTER_ID: "MkU3OEVBNTcwNTJENDM2Qk"
    volumes:
      - kafka-data:/var/lib/kafka/data
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "bash -c '</dev/tcp/127.0.0.1/9094'" ]
      interval: 5s
      timeout: 3s
      retries: 30

  kafka-schema-registry:
    image: confluentinc/cp-schema-registry:8.1.0
    container_name: kafka-schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "PLAINTEXT://kafka:9092"
      SCHEMA_REGISTRY_LISTENERS: "http://0.0.0.0:8081"
      SCHEMA_REGISTRY_HOST_NAME: "kafka-schema-registry"
    ports:
      - "8081:8081"
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://localhost:8081/subjects >/dev/null" ]
      interval: 5s
      timeout: 5s
      retries: 20
      start_period: 15s

  kafka-ui:
    image: kafbat/kafka-ui:latest
    depends_on:
      kafka:
        condition: service_healthy
      kafka-schema-registry:
        condition: service_healthy
      connect:
        condition: service_healthy
    environment:
      KAFKA_CLUSTERS_0_NAME: "local"
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "kafka:9092"
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: "http://kafka-schema-registry:8081"
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: "connect-local"
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: "http://connect:8083"
      # optional: enable runtime config editor in UI
      # DYNAMIC_CONFIG_ENABLED: "true"
    ports:
      - "8080:8080"

  connect:
    image: confluentinc/cp-kafka-connect:8.1.0
    container_name: kafka-connect
    hostname: kafka-connect
    depends_on:
      - kafka
      - redis
      - kafka-schema-registry
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka:9092"
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_GROUP_ID: "connect-cluster"

      CONNECT_CONFIG_STORAGE_TOPIC: "__connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "__connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "__connect-status"

      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1

      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://kafka-schema-registry:8081"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"

      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"

      CONNECT_PLUGIN_PATH: "/usr/share/confluent-hub-components"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"

      AWS_ACCESS_KEY_ID: "admin"
      AWS_SECRET_ACCESS_KEY: "password"
      AWS_REGION: "us-east-1"
      AWS_S3_ENDPOINT: "http://minio:9000"
    ports:
      - "8083:8083"
    volumes:
      - ./connect-plugins:/usr/share/confluent-hub-components
      - ./data-dumps:/data/dumps
    restart: unless-stopped

  postgres:
    image: postgis/postgis:18-3.6
    container_name: postgres
    environment:
      POSTGRES_DB: gtfs
      POSTGRES_USER: app
      POSTGRES_PASSWORD: app
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U app -d gtfs" ]
      interval: 5s
      timeout: 5s
      retries: 20
      start_period: 20s

  redis:
    image: redis:7.4
    container_name: redis
    ports:
      - "6379:6379"
    restart: unless-stopped

  arrival-detector:
    build: ../arrival-detector
    container_name: arrival-detector
    depends_on:
      kafka:
        condition: service_healthy
      kafka-schema-registry:
        condition: service_healthy
      gtfs-backend:
        condition: service_started
    environment:
      KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
      SCHEMA_REGISTRY_URL: "http://kafka-schema-registry:8081"
      GTFS_BACKEND_URL: "http://gtfs-backend:8080"

      INPUT_TOPIC: "gtfs.vehicle.positions.sl"
      OUTPUT_TOPIC: "stop_arrivals"
    restart: unless-stopped

  gtfs-backend:
    build: # path to Dockerfile
    container_name: gtfs-backend
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started

    environment:
      SERVER_PORT: "8080"

      SPRING_DATASOURCE_URL: "jdbc:postgresql://postgres:5432/gtfs"
      SPRING_DATASOURCE_USERNAME: "app"
      SPRING_DATASOURCE_PASSWORD: "app"

      SPRING_REDIS_HOST: "redis"
      SPRING_REDIS_PORT: "6379"

      VEHPOS_RECENCY_SECONDS: "600"
      VEHPOS_TRIP_PREFIX: "vehpos:SL:"

    ports:
      - "8082:8080"
    restart: unless-stopped

  web:
    build:
      context: # path to Dockerfile
      args:
        VITE_IOTDP_BACKEND_URL: "http://localhost:8082/api"
    container_name: web
    depends_on:
      gtfs-backend:
        condition: service_started
    ports:
      - "5173:80"
    restart: unless-stopped

  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password
      MINIO_DOMAIN: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    networks:
      default:
        aliases:
          - gtfs.minio
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:9000/minio/health/live || exit 1" ]
      interval: 5s
      timeout: 5s
      retries: 20
      start_period: 10s

  minio-init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    entrypoint:
      - /bin/sh
      - -c
      - |
        mc alias set local http://minio:9000 admin password &&
        mc mb --ignore-existing local/gtfs &&
        mc ls local &&
        echo "Bucket gtfs ready."
    restart: "no"

  postgres-rest:
    image: postgres:16-alpine
    container_name: postgres-rest
    environment:
      POSTGRES_DB: iceberg
      POSTGRES_USER: iceberg
      POSTGRES_PASSWORD: iceberg
    ports:
      - "5433:5432"
    volumes:
      - pg-data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U iceberg -d iceberg" ]
      interval: 3s
      timeout: 2s
      retries: 50

  iceberg-rest:
    image: tabulario/iceberg-rest:latest
    container_name: iceberg-rest
    depends_on:
      postgres-rest:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    ports:
      - "8181:8181"
    environment:
      CATALOG_CATALOG__IMPL: org.apache.iceberg.jdbc.JdbcCatalog
      CATALOG_URI: jdbc:postgresql://postgres-rest:5432/iceberg
      CATALOG_JDBC_USER: iceberg
      CATALOG_JDBC_PASSWORD: iceberg
      CATALOG_JDBC_DRIVER: org.postgresql.Driver
      CATALOG_WAREHOUSE: s3://gtfs/

      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO

      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_REGION: us-east-1
      CATALOG_S3_PATH_STYLE_ACCESS: "true"

      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password
      AWS_REGION: us-east-1

  trino:
    image: trinodb/trino:latest
    container_name: trino
    depends_on:
      iceberg-rest:
        condition: service_started
      minio:
        condition: service_healthy
      postgres:
        condition: service_healthy
    ports:
      - "8085:8080"
    volumes:
      - ./trino/config.properties:/etc/trino/config.properties:ro
      - ./trino/jvm.config:/etc/trino/jvm.config:ro
      - ./trino/node.properties:/etc/trino/node.properties:ro
      - ./trino/catalog/:/etc/trino/catalog/:ro

volumes:
  kafka-data:
  postgres-data:
  minio-data:
  pg-data:
